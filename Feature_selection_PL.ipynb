{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the function for feature selection using similarity measure and fuzzy entroropy measures based on the article: \n",
    "P. Luukka, (2011) Feature Selection Using Fuzzy Entropy Measures with Similarity Classifier, Expert Systems with Applications, 38, pp. 4600-4607"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_sim(data, measure = 'luca', p = 1):\n",
    "    \n",
    "    # Feature selection method using similarity measure and fuzzy entroropy \n",
    "    # measures based on the article:\n",
    "\n",
    "    # P. Luukka, (2011) Feature Selection Using Fuzzy Entropy Measures with\n",
    "    # Similarity Classifier, Expert Systems with Applications, 38, pp. 4600-4607\n",
    "\n",
    "    # Function call:\n",
    "    # feature_selection_sim(data, measure, p)\n",
    "\n",
    "    # OUTPUTS:\n",
    "    # data_mod      data with removed feature\n",
    "    # index_rem     index of removed feature in original data\n",
    "\n",
    "    # INPUTS:\n",
    "    # data          dataframe, contains class values\n",
    "    # measure       fuzzy entropy measure, either 'luca' or 'park'              \n",
    "    #               currently coded\n",
    "    # p             parameter of Lukasiewicz similarity measure\n",
    "    #               p in (0, \\infty) as default p=1.\n",
    "    \n",
    "    # You need to import 'numpy' as 'np' before using this function\n",
    "\n",
    "    l = int(max(data.iloc[:,-1]))   # -classes in the last column\n",
    "    m = data.shape[0]               # -samples\n",
    "    t = data.shape[1]-1             # -features\n",
    "    \n",
    "    dataold = data.copy()\n",
    "    \n",
    "    idealvec_s = np.zeros((l,t)) \n",
    "    for k in range(l):\n",
    "        idx = data.iloc[:,-1] == k+1\n",
    "        idealvec_s[k,:] = data[idx].iloc[:,:-1].mean(axis = 0)\n",
    "    \n",
    "    # scaling data between [0,1]\n",
    "    data_v = data.iloc[:,:-1]\n",
    "    data_c = data.iloc[:,-1] # labels\n",
    "    mins_v = data_v.min(axis = 0)\n",
    "    Ones   = np.ones((data_v.shape))\n",
    "    data_v = data_v + np.dot(Ones,np.diag(abs(mins_v)))\n",
    "    \n",
    "    tmp =[]\n",
    "    for k in range(l):\n",
    "        tmp.append(abs(mins_v))\n",
    "    \n",
    "    idealvec_s = idealvec_s+tmp\n",
    "    maxs_v     = data_v.max(axis = 0)\n",
    "    data_v     = np.dot(data_v,np.diag(maxs_v**(-1)))\n",
    "    tmp2 =[];\n",
    "    for k in range(l):\n",
    "        tmp2.append(abs(maxs_v))\n",
    "        \n",
    "    idealvec_s = idealvec_s/tmp2\n",
    "    \n",
    "    data_vv = pd.DataFrame(data_v) # Convert the array of feature to a dataframe\n",
    "    data    = pd.concat([data_vv, data_c], axis=1, ignore_index=False)\n",
    "\n",
    "    # sample data\n",
    "    datalearn_s = data.iloc[:,:-1]\n",
    "    \n",
    "    # similarities\n",
    "    sim = np.zeros((t,m,l))\n",
    "    \n",
    "    for j in range(m):\n",
    "        for i in range(t):\n",
    "            for k in range(l):\n",
    "                sim[i,j,k] = (1-abs(idealvec_s[k,i]**p - datalearn_s.iloc[j,i])**p)**(1/p)\n",
    "            \n",
    "    sim = sim.reshape(t,m*l)\n",
    "    \n",
    "    # possibility for two different entropy measures\n",
    "    if measure =='luca':\n",
    "        # moodifying zero and one values of the similarity values to work with \n",
    "        # De Luca's entropy measure\n",
    "        delta = 1e-10\n",
    "        sim[sim == 1] = delta\n",
    "        sim[sim == 0] = 1-delta\n",
    "        H = (-sim*np.log(sim)-(1-sim)*np.log(1-sim)).sum(axis = 1)\n",
    "    elif measure == 'park':\n",
    "        H = (np.sin(np.pi/2*sim)+np.sin(np.pi/2*(1-sim))-1).sum(axis = 1) \n",
    "        \n",
    "    # find maximum feature\n",
    "    max_idx = np.argmax(H) # notice that index is starting from 0\n",
    "    \n",
    "    #removing feature from the data\n",
    "    data_mod = dataold.drop(dataold.columns[max_idx], axis=1)\n",
    "    \n",
    "    return max_idx, data_mod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the 'feature_selection_sim' function with an example dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "p = 1\n",
    "measure = 'luca'\n",
    "\n",
    "data = [[0.4600,    0.3400,    0.1400,    0.0300,    0.9218,    1.0000],\n",
    "        [0.5000,    0.3400,    0.1500,    0.0200,    0.7382,    1.0000],\n",
    "        [0.4400,    0.2900,    0.1400,    0.0200,    0.1763,    1.0000],\n",
    "        [0.7600,    0.3000,    0.6600,    0.2100,    0.4057,    2.0000],\n",
    "        [0.4900,    0.2500,    0.4500,    0.1700,    0.9355,    2.0000],\n",
    "        [0.7300,    0.2900,    0.6300,    0.1800,    0.9169,    2.0000]]\n",
    "\n",
    "data = pd.DataFrame(data) # convert array data to a dataframe\n",
    "    \n",
    "idx, datanew = feature_selection_sim(data,measure,p) # function call\n",
    "\n",
    "# idx: index of the removed feature\n",
    "# datanew: data with removed feature\n",
    "\n",
    "# data.columns[idx] # name of the removed feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the function with Iris dataset (you need to import datasets from from sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Load iris training dataset\n",
    "X = iris.data\n",
    "\n",
    "# Load iris target set\n",
    "Y = iris.target\n",
    "\n",
    "# Convert datasets' type into dataframe\n",
    "df_f = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df_c = pd.DataFrame(Y)\n",
    "\n",
    "data = pd.concat([df_f, df_c], axis=1, ignore_index=False)\n",
    "\n",
    "idx, datanew = feature_selection_sim(data)\n",
    "\n",
    "# data.columns[idx] : name of the removed feature\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
